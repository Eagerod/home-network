# Pile of anchors to use just so configurable values are all kept in one spot.
# Could eventually be replaced with something like a constants/variables key
#   that could live here, but this does what it needs to.
_:
  - &log_level trace
  - &metallb_address_range ADDRESS_RANGE=192.168.200.32-192.168.200.224
  - &ingress_controller_service_ip LOAD_BALANCER_IP=192.168.200.64
  - &ingress_controller_external_service_ip LOAD_BALANCER_IP=192.168.200.129
  - &plex_load_balancer_service_ip LOAD_BALANCER_IP=192.168.200.69
  - &wireguard_load_balancer_service_ip LOAD_BALANCER_IP=192.168.200.123
  - &pihole_ingress_controller_ip INGRESS_IP=192.168.200.64
  - &unifi_controller_service_ip LOAD_BALANCER_IP=192.168.200.82
  - &unifi_controller_inform_url http://192.168.200.82:8080
  - &docker_hub_upstream_pull if-not-present
  - &local_upstream_pull always
access_point_controller: *unifi_controller_inform_url
access_points:
  - ubnt@192.168.1.86
  - ubnt@192.168.1.109
  - ubnt@192.168.1.152
vms:
  root: vms
  cache: /var/lib/packer/cache
  output: /var/lib/packer/images
  images:
    - name: load-balancer
      parameters:
        - ESXI_ROOT_PASSWORD
        - SERVER_SSH_KEYS_DIR=/var/lib/packer/etc/ssh
        - ESXI_NETWORK=VM Network
        - VM_ROOT_PASSWORD
        - VM_MANAGEMENT_PASSWORD
    - name: kubernetes-node
      parameters:
        - ESXI_ROOT_PASSWORD
        - SERVER_SSH_KEYS_DIR=/var/lib/packer/etc/ssh
        - ESXI_NETWORK=VM Network
        - VM_ROOT_PASSWORD
        - VM_MANAGEMENT_PASSWORD
load_balancer_host: api.internal.aleemhaji.com
nodes:
  - name: beast1
    role: hypervisor
    engine: esxi
    host: 192.168.10.40
    user: root
    datastore: Main
    network: Kubernetes Network
  - name: beast2
    role: hypervisor
    engine: esxi
    host: 192.168.10.41
    user: root
    datastore: Main
    network: Kubernetes Network
  - name: home-load-balancer
    role: load-balancer
    hypervisor: beast1
    user: packer
  - name: home-master-01
    role: master
    hypervisor: beast1
    user: packer
  - name: home-master-02
    role: master
    hypervisor: beast1
    user: packer
  - name: home-master-03
    role: master
    hypervisor: beast1
    user: packer
  - name: home-node-01
    role: node
    hypervisor: beast1
    user: packer
  - name: home-node-02
    role: node
    hypervisor: beast1
    user: packer
  - name: home-node-03
    role: node
    hypervisor: beast1
    user: packer
  - name: home-node-04
    role: node
    hypervisor: beast1
    user: packer
  - name: home-node-05
    role: node
    hypervisor: beast1
    user: packer
  - name: home-node-06
    role: node
    hypervisor: beast1
    user: packer
  # Provide extra capacity while cycling other nodes.
  - name: home-node-temp
    role: node
    hypervisor: beast1
    user: packer
  - name: home-master-temp
    role: master
    hypervisor: beast1
    user: packer
loglevel: *log_level
pod_network_cidr: 10.244.0.0/16
resources:
  - name: calico
    file: https://docs.projectcalico.org/archive/v3.18/manifests/calico.yaml
    tags: [network]
  - name: dashboard
    file: https://raw.githubusercontent.com/kubernetes/dashboard/v2.1.0/aio/deploy/recommended.yaml
    tags: [apps, dashboard]
  - name: metrics-server
    file: metrics-server.yaml
  - name: personal-service-account
    inline: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: aleem
        namespace: kube-system
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: aleem
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-admin
      subjects:
      - kind: ServiceAccount
        name: aleem
        namespace: kube-system
    tags: [users]
  - name: load-balancer-namespace
    file: https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml
    tags: [network, load-balancer]
  - name: load-balancer
    file: metallb-ifnotpresent.yaml
    tags: [network, load-balancer]
  - name: load-balancer-config
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        namespace: metallb-system
        name: config
      data:
        config: |
          address-pools:
            - name: default
              protocol: layer2
              addresses:
                - ${ADDRESS_RANGE}
      ---
      apiVersion: v1
      data:
        secretkey: "${METALLB_SYSTEM_MEMBERLIST_SECRET_KEY_BASE64}"
      kind: Secret
      metadata:
        name: memberlist
        namespace: metallb-system
    parameters:
      - METALLB_SYSTEM_MEMBERLIST_SECRET_KEY_BASE64
      - *metallb_address_range
    tags: [network, load-balancer]
  - name: ingress-controller
    file: nginx-ingress-daemonset-plus-tcp-udp-proxy.yaml
    tags: [network, ingress, ingress-internal]
  - name: ingress-controller-external
    file: nginx-ingress-daemonset-external.yaml
    tags: [network, ingress, ingress-external]
  - name: ingress-controller-configs
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: tcp-services
      data:
        22: "default/gitea:22"
        53: "default/pihole-tcp:53"
        3306: "default/mysql:3306"
        5432: "default/postgres:5432"
        6379: "default/redis:6379"
        27017: "default/mongodb:27017"
        51413: "default/osstransmission:51413"
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: udp-services
      data:
        53: "default/pihole-udp:53"
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx
      data:
        http-redirect-code: "302"
        enable-real-ip: "true"
    tags: [network, ingress, ingress-internal]
  - name: ingress-controller-configs-external
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx-external
      data:
        http-redirect-code: "302"
        enable-real-ip: "true"
    tags: [network, ingress, ingress-external]
  - name: ingress-controller-service
    inline: |
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-ingress-tcp
        labels:
          app: nginx-ingress
        namespace: ingress-nginx
        annotations:
          metallb.universe.tf/allow-shared-ip: ingress
      spec:
        externalTrafficPolicy: Local
        ports:
        - port: 22
          name: gitea-ssh
          protocol: TCP
        - port: 53
          name: dns
          protocol: TCP
        - port: 80
          name: http
          protocol: TCP
        - port: 443
          name: https
          protocol: TCP
        - port: 3306
          name: mysql
          protocol: TCP
        - port: 5432
          name: postgres
          protocol: TCP
        - port: 6379
          name: redis
          protocol: TCP
        - port: 27017
          name: mongodb
          protocol: TCP
        - port: 51413
          name: osstransmission
          protocol: TCP
        type: LoadBalancer
        loadBalancerIP: ${LOAD_BALANCER_IP}
        selector:
          app.kubernetes.io/name: ingress-nginx
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-ingress-udp
        labels:
          app: nginx-ingress
        namespace: ingress-nginx
        annotations:
          metallb.universe.tf/allow-shared-ip: ingress
      spec:
        externalTrafficPolicy: Local
        ports:
        - port: 53
          name: dns
          protocol: UDP
        type: LoadBalancer
        loadBalancerIP: ${LOAD_BALANCER_IP}
        selector:
          app.kubernetes.io/name: ingress-nginx
    tags: [network, ingress, ingress-internal]
    parameters:
      - *ingress_controller_service_ip
  - name: ingress-controller-service-external
    inline: |
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-ingress-tcp
        labels:
          app: nginx-ingress
        namespace: ingress-nginx-external
        annotations:
          metallb.universe.tf/allow-shared-ip: ingress-external
      spec:
        externalTrafficPolicy: Local
        ports:
        - port: 80
          name: http
          protocol: TCP
        - port: 443
          name: https
          protocol: TCP
        type: LoadBalancer
        loadBalancerIP: ${LOAD_BALANCER_IP}
        selector:
          app.kubernetes.io/name: ingress-nginx
      # ---
      # apiVersion: v1
      # kind: Service
      # metadata:
      #   name: nginx-ingress-udp
      #   labels:
      #     app: nginx-ingress
      #   namespace: ingress-nginx-external
      #   annotations:
      #     metallb.universe.tf/allow-shared-ip: ingress-external
      # spec:
      #   externalTrafficPolicy: Local
      #   ports:
      #   type: LoadBalancer
      #   loadBalancerIP: ${LOAD_BALANCER_IP}
      #   selector:
      #     app.kubernetes.io/name: ingress-nginx
    tags: [network, ingress, ingress-external]
    parameters:
      - *ingress_controller_external_service_ip
  - name: kubernetes-dashboard-ingress
    inline: |
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        namespace: kubernetes-dashboard
        name: dashboard-ingress-redirect
        annotations:
          kubernetes.io/ingress.class: "nginx"
          nginx.ingress.kubernetes.io/temporal-redirect: https://dashboard.internal.aleemhaji.com
      spec:
        rules:
          - host: dashboard
          - host: dashboard.home
      ---
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        namespace: kubernetes-dashboard
        name: dashboard-ingress
        annotations:
          kubernetes.io/ingress.class: "nginx"
          nginx.ingress.kubernetes.io/backend-protocol: "https"
      spec:
        tls:
          - hosts:
              - dashboard.internal.aleemhaji.com
            secretName: internal-certificate-files
        rules:
          - host: dashboard.internal.aleemhaji.com
            http:
              paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: kubernetes-dashboard
                      port:
                        number: 443
    tags: [apps, dashboard]
  - name: cluster-registry-secrets-default
    file: registry/registry-secrets.yaml
    parameters:
      - NAMESPACE=default
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
    tags: [apps, registry]
  - name: cluster-registry-secrets-dev
    file: registry/registry-secrets.yaml
    parameters:
      - NAMESPACE=dev
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
    tags: [apps, registry]
  - name: cluster-registry-secrets-monitoring
    file: registry/registry-secrets.yaml
    parameters:
      - NAMESPACE=monitoring
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
    tags: [apps, registry]
  - name: cluster-registry-secrets-kube-system
    file: registry/registry-secrets.yaml
    parameters:
      - NAMESPACE=kube-system
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
    tags: [apps, registry]
  - name: docker-registry-htpasswd-secrets
    inline: |
      apiVersion: v1
      data:
        htpasswd: ${DOCKER_HTPASSWD_FILE_CONTENTS_BASE64}
      kind: Secret
      metadata:
        name: ${DOCKER_REGISTRY_HOSTNAME}-htpasswd
    parameters:
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_HTPASSWD_FILE_CONTENTS_BASE64
    tags: [apps, registry]
  - name: docker-registry
    file: registry/registry.yaml
    tags: [apps, registry]
  # region: Docker Image Caching
  # At this point, the registry has been pushed, and all the appropriate
  #   ingress configurations should be set to allow for images to be cached.
  # Cache a bunch of images from Docker Hub to the local registry to prevent
  #   being rate limited during standard operations within the cluster.
  # The main contributor to Docker Hub pulls may just be the crons, but if the
  #   cluster has tried enough pulls within the last 6 hours, just doing
  #   rudimentary dev work gets blocked.
  # Rehosting the images on the cluster also helps steer things towards images
  #   being built from same base image any time the same tag appears, rather
  #   it being based on what base image the build host had at the time.
  # Similar issues are possible, but are a little more controlled.
  - name: ubuntu-1604-image-cache
    build:
      source: ubuntu:16.04
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/ubuntu:16.04
    tags: [docker-cache, crons]
  - name: ubuntu-2004-image-cache
    build:
      source: ubuntu:20.04
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/ubuntu:20.04
    tags: [docker-cache, transmission]
  - name: certbot-image-cache
    build:
      source: certbot/certbot:v1.31.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/certbot:v1.31.0
    tags: [docker-cache, certbot]
  - name: mongo-37-image-cache
    build:
      source: mongo:3.7
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/mongo:3.7
    tags: [docker-cache, mongodb]
  - name: mongo-36-image-cache
    build:
      source: mongo:3.6
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/mongo:3.6
    tags: [docker-cache, mongodb, unifi]
  - name: busybox-image-cache
    build:
      source: busybox:1.35.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/busybox:1.35.0
    tags: [docker-cache, mysql, redis, pihole]
  - name: mysql-57-image-cache
    build:
      source: mysql:5.7.37
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/mysql:5.7.37
    tags: [docker-cache, mysql]
  - name: pihole-image-cache
    build:
      source: pihole/pihole:v5.8
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/pihole:v5.8
    tags: [docker-cache, pihole]
  - name: browser-image-cache
    build:
      source: klausmeyer/docker-registry-browser:1.5.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/docker-registry-browser:1.5.0
    tags: [docker-cache, browser]
  - name: curl-cache
    build:
      source: curlimages/curl:7.80.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/curl:7.80.0
    tags: [docker-cache, remindmebot, webcomics]
  - name: postgres-image-cache
    build:
      source: postgres:9
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/postgres:9
    tags: [docker-cache, postgres]
  # This image has been pulled from dockerhub, so it has to be pulled from the
  #   local registry, and then re-tagged with the original tag for now.
  - name: firefly-image-cache
    build:
      source: fireflyiii/core:version-5.5.13
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/firefly-iii:5.5.13
    tags: [docker-cache, firefly]
  - name: heimdall-image-cache
    build:
      source: linuxserver/heimdall:version-v2.4.13
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/heimdall:version-v2.4.13
    tags: [docker-cache, heimdall]
  - name: plex-image-cache
    build:
      source: plexinc/pms-docker:1.29.0.6244-819d3678c
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/pms-docker:1.29.0.6244-819d3678c
    tags: [docker-cache, plex]
  - name: jellyfin-image-cache
    build:
      source: jellyfin/jellyfin:10.8.6
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/jellyfin:10.8.6
    tags: [docker-cache, jellyfin]
  - name: redis-image-cache
    build:
      source: redis:5
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/redis:5
    tags: [docker-cache, redis]
  - name: guacamole-image-cache
    build:
      source: guacamole/guacamole:1.4.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/guacamole:1.4.0
    tags: [docker-cache, guacamole]
  - name: guacd-image-cache
    build:
      source: guacamole/guacd:1.4.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/guacd:1.4.0
    tags: [docker-cache, guacamole]
  - name: gitea-image-cache
    build:
      source: gitea/gitea:1.17.3
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/gitea:1.17.3
    tags: [docker-cache, gitea]
  - name: kubectl-image-cache
    build:
      source: bitnami/kubectl:1.21.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/kubectl:1.21.0
    tags: [docker-cache, kubectl]
  - name: bitwarden-image-cache
    build:
      source: vaultwarden/server:1.26.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/vaultwarden/server:1.26.0
    tags: [docker-cache, bitwarden, vaultwarden]
  - name: syncthing-image-cache
    build:
      source: syncthing/syncthing:1.22
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/syncthing:1.22
    tags: [docker-cache, syncthing]
  - name: bookstack-image-cache
    build:
      source: linuxserver/bookstack:22.09.1
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/bookstack:22.09.1
    tags: [docker-cache, bookstack]
  - name: drone-image-cache
    build:
      source: drone/drone:2.15.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/drone/drone:2.15.0
    tags: [docker-cache, drone]
  - name: drone-agent-image-cache
    build:
      source: drone/drone-runner-kube:1.0.0-rc.3
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/drone/drone-runner-kube:1.0.0-rc.3
    tags: [docker-cache, drone]
  - name: wireguard-image-cache
    build:
      source: masipcat/wireguard-go:0.0.20220316
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/wireguard-go:0.0.20220316
    tags: [docker-cache, wireguard]
  - name: unifi-controller-image-cache
    build:
      source: linuxserver/unifi-controller:7.2.94
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/unifi-controller:7.2.94
    tags: [docker-cache, unifi]
  # endregion
  - name: pihole
    file: pihole/pihole.yaml
    parameters:
      - *pihole_ingress_controller_ip
    tags: [apps, pihole]
  # https://github.com/prometheus-operator/kube-prometheus#quickstart
  - name: prometheus
    file: opsbox.yaml
    parameters:
      - JOB_NAME=deploy-prometheus
      - SCRIPT=cd /tmp; curl -fsSL https://github.com/prometheus-operator/kube-prometheus/archive/v0.7.0.tar.gz | tar xvz; cd kube-prometheus-0.7.0; kubectl create -f manifests/setup; until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ""; done; kubectl create -f manifests;
  - name: grafana
    file: grafana/grafana.yaml
  # region: Stateless Services
  # The set of services in this block are services that don't rely on storing
  #   any state on disk.
  # These are grouped together so that they can be easily be pushed out to
  #   parallel clusters first without having any concern for services fighting
  #   over shares/volumes.
  # They should each have the stateless tag applied to them.
  - name: image-monitor-image
    build:
      path: image-monitor
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/image-monitor:latest
    tags: [apps, slackbot, stateless]
  - name: image-monitor-config
    inline: |
      apiVersion: v1
      binaryData:
        image-monitor.sh: ${IMAGE_MONITOR_SCRIPT}
        image-monitor-ignore.json: ${IMAGE_MONITOR_IGNORE_JSON}
      kind: ConfigMap
      metadata:
        name: image-monitor-config
    fileParameters:
      - IMAGE_MONITOR_SCRIPT=./image-monitor/image-monitor.sh
      - IMAGE_MONITOR_IGNORE_JSON=./image-monitor/image-monitor-ignore.json
    tags: [crons, image-monitor, stateless]
  - name: image-monitor
    file: image-monitor/image-monitor.yaml
    tags: [crons, image-monitor, stateless]
  - name: browser
    file: browser/browser.yaml
    tags: [apps, browser, stateless]
  - name: slackbot-image
    build:
      path: slackbot
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/slackbot:latest
    tags: [apps, slackbot, stateless]
  - name: slackbot-config
    inline: |
      apiVersion: v1
      data:
        alerting_channel: ${SLACK_BOT_ALERTING_CHANNEL}
        default_channel: ${SLACK_BOT_DEFAULT_CHANNEL}
      kind: ConfigMap
      metadata:
        name: slack-bot-config
      ---
      apiVersion: v1
      data:
        api_key: ${SLACK_BOT_API_KEY_BASE64}
      kind: Secret
      metadata:
        name: slack-bot-secrets
    parameters:
      - SLACK_BOT_ALERTING_CHANNEL
      - SLACK_BOT_DEFAULT_CHANNEL
      - SLACK_BOT_API_KEY_BASE64
    tags: [apps, slackbot, stateless]
  - name: slackbot
    file: slackbot/slackbot.yaml
    tags: [apps, slackbot, stateless]
  - name: redis
    file: redis/redis.yaml
    tags: [apps, redis, stateless]
  - name: dns-update-secrets
    inline: |
      apiVersion: v1
      data:
        value: ${NAMESILO_API_KEY_BASE64}
      kind: Secret
      metadata:
        name: namesilo-api-key
    parameters:
      - NAMESILO_API_KEY_BASE64
    tags: [crons, dns, certbot, stateless]
  - name: dns-update-cron
    file: crons/dns-cron.yaml
    tags: [crons, dns, stateless]
  - name: latex-image
    build:
      path: latex
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/latex-server:latest
    tags: [apps, latex, stateless]
  - name: latex-server
    file: latex/latex.yaml
    tags: [apps, latex, stateless]
  # endregion
  # region: NFS-Share Only Services
  # These services are grouped because they have a pretty small number of
  #   dependencies outside the cluster itself.
  # These can easily be stopped in one cluster, and started in another without
  #   the need to coordinate ordering with other services in the cluster.
  # Disks management is also entirely outside the responsibility of these
  #   pods, so they're a bit lower maintenance.
  # This block should only list services that are internally-accessible.
  # Anything that's exposed publicly shouldn't be considered NFS-share only.
  - name: heimdall
    file: heimdall/heimdall.yaml
    tags: [apps, heimdall]
  - name: tedbot-config
    inline: |
      apiVersion: v1
      data:
        slack_webhook_url: ${SLACK_TEDBOT_APP_WEBHOOK_BASE64}
      kind: Secret
      metadata:
        name: tedbot-secrets
    parameters:
      - SLACK_TEDBOT_APP_WEBHOOK_BASE64
    tags: [crons, tedbot]
  - name: tedbot
    file: tedbot/tedbot.yaml
    tags: [crons, tedbot]
  - name: gitea
    file: gitea/gitea.yaml
    tags: [apps, gitea]
  - name: bitwarden-config
    inline: |
      apiVersion: v1
      data:
        admin_token: ${BITWARDEN_ADMIN_TOKEN_BASE64}
      kind: Secret
      metadata:
        name: bitwarden-secrets
    parameters:
      - BITWARDEN_ADMIN_TOKEN_BASE64
    tags: [apps, bitwarden, vaultwarden]
  - name: bitwarden
    file: bitwarden/bitwarden.yaml
    tags: [apps, bitwarden, vaultwarden]
  # endregion
  # region: NFS-Share + Port Forwarded Services
  # This block of services include resources that need access to shared
  #   storage, but also have a port forward configured for them.
  # Deploying these resources will require a little bit of extra care, as they
  #   will be temporarily unreachable on whatever port they're talking
  #   externally through.
  # These shouldn't have any database dependencies, but can have NFS and
  #   ingress.
  - name: transmission-image
    build:
      path: transmission
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/transmission:2004
    tags: [apps, transmission]
  - name: osstransmission
    file: transmission/transmission.yaml
    tags: [apps, transmission]
  - name: plex
    file: plex/plex.yaml
    parameters:
      - *plex_load_balancer_service_ip
    tags: [apps, plex]
  - name: jellyfin
    file: jellyfin/jellyfin.yaml
    tags: [apps, jellyfin]
  - name: wireguard
    file: wireguard/wireguard.yaml
    parameters:
      - *wireguard_load_balancer_service_ip
      - WIREGUARD_SERVER_IP=172.27.224.1
      - WIREGUARD_CLIENT_IP_CIDR=172.27.224.0/24
      - WIREGUARD_SERVER_PRIVATE_KEY
      - WIREGUARD_PEER_1_PUBLIC_KEY
      - WIREGUARD_PEER_1_IP=172.27.224.32/32
    tags: [apps, wireguard]
  # endregion
  # region: Databases
  # This section contains definitions for databases.
  # These resources will typically need to refer to block storage that's been
  #   exposed via another piece of hardware that's outside the scope of hope,
  #   for now.
  # These resources will be consumed by others, so they appear in this file
  #   before any of the services that consume them.
  # The number of dependencies they have may make migrations a bit more of a
  #   challenge.
  - name: postgres-config
    inline: |
      apiVersion: v1
      data:
        root_password: ${POSTGRES_ROOT_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: postgres-secrets
    parameters:
      - POSTGRES_ROOT_PASSWORD_BASE64
    tags: [apps, postgres]
  - name: postgres
    file: postgres/postgres.yaml
    tags: [apps, postgres]
  - name: mysql-config
    inline: |
      apiVersion: v1
      data:
        root_password: ${MYSQL_ROOT_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: mysql-secrets
    parameters:
      - MYSQL_ROOT_PASSWORD_BASE64
    tags: [apps, mysql]
  - name: mysql-backup
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mysql-backup
      binaryData:
        mysql-backup.sh: ${MYSQL_BACKUP_SCRIPT}
    fileParameters:
      - MYSQL_BACKUP_SCRIPT=mysql/mysql-backup.sh
    tags: [apps, mysql, backups]
  - name: mysql
    file: mysql/mysql.yaml
    tags: [apps, mysql]
  - name: mongodb-backup
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mongodb-backup
      binaryData:
        mongodb-backup.sh: ${MONGODB_BACKUP_SCRIPT}
    tags: [apps, mongodb, backups]
    fileParameters:
      - MONGODB_BACKUP_SCRIPT=mongodb/mongodb-backup.sh
  - name: mongodb
    file: mongodb/mongodb.yaml
    tags: [apps, mongodb]
  # endregion
  # region: Database-Dependent Services
  # This section contains the listing of services that rely on some form of
  #   datastore outside of just file system.
  # This could come in the form of network references to resources defined in
  #   the databases section above, or in the form of a database defined in its
  #   own pod that requires a special ISCSi device set up for it.
  - name: unifi-controller-image
    build:
      path: unifi
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/unifi-controller:7.2.94-custom
    tags: [apps, unifi]
  - name: unifi-controller
    file: unifi/unifi.yaml
    parameters:
      - *unifi_controller_service_ip
    tags: [apps, unifi]
  - name: firefly-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${FIREFLY_MYSQL_USER}
        mysql_database: ${FIREFLY_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: firefly-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${FIREFLY_MYSQL_PASSWORD_BASE64}
        app_key: ${FIREFLY_APP_KEY_BASE64}
      kind: Secret
      metadata:
        name: firefly-secrets
    parameters:
      - FIREFLY_MYSQL_USER
      - FIREFLY_MYSQL_DATABASE
      - FIREFLY_MYSQL_PASSWORD_BASE64
      - FIREFLY_APP_KEY_BASE64
    tags: [apps, firefly]
  - name: firefly-init-job-create
    file: firefly/firefly-init.yaml
    tags: [apps, firefly]
  - name: firefly-init-job
    job: firefly-mysql-init
    tags: [apps, firefly]
  - name: firefly
    file: firefly/firefly.yaml
    tags: [apps, firefly]
  - name: guacamole-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${GUACAMOLE_MYSQL_USER}
        mysql_database: ${GUACAMOLE_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: guacamole-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${GUACAMOLE_MYSQL_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: guacamole-secrets
    parameters:
      - GUACAMOLE_MYSQL_USER
      - GUACAMOLE_MYSQL_DATABASE
      - GUACAMOLE_MYSQL_PASSWORD_BASE64
    tags: [apps, guacamole]
  - name: guacamole-init-job-create
    file: guacamole/guacamole-init.yaml
    tags: [apps, guacamole]
  - name: guacamole-init-job
    job: guacamole-mysql-init
    tags: [apps, guacamole]
  - name: guacamole
    file: guacamole/guacamole.yaml
    tags: [apps, guacamole]
  - name: node-red-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${NODE_RED_MYSQL_USER}
        mysql_database: ${NODE_RED_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: node-red-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${NODE_RED_MYSQL_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: node-red-secrets
    parameters:
      - NODE_RED_MYSQL_USER
      - NODE_RED_MYSQL_DATABASE
      - NODE_RED_MYSQL_PASSWORD_BASE64
    tags: [apps, node-red]
  - name: node-red-image
    build:
      path: node-red
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/node-red:latest
    tags: [apps, node-red]
  - name: node-red-init-job-create
    file: node-red/node-red-init.yaml
    tags: [apps, node-red]
  - name: node-red-init-job
    job: node-red-mysql-init
    tags: [apps, node-red]
  - name: node-red
    file: node-red/node-red.yaml
    tags: [apps, node-red]
  - name: remindmebot-config
    inline: |
      apiVersion: v1
      data:
        bot_username: ${REMINDMEBOT_USERNAME}
      kind: ConfigMap
      metadata:
        name: remindmebot-config
      ---
      apiVersion: v1
      data:
        bot_api_key: ${REMINDMEBOT_API_KEY_BASE64}
        database: ${REMINDMEBOT_DATABASE_BASE64}
      kind: Secret
      metadata:
        name: remindmebot-secrets
    parameters:
      - REMINDMEBOT_USERNAME
      - REMINDMEBOT_API_KEY_BASE64
      - REMINDMEBOT_DATABASE_BASE64
    tags: [apps, remindmebot]
  - name: remindmebot-init-job-start
    file: remindmebot/remindmebot-init.yaml
    tags: [apps, remindmebot]
  - name: remindmebot-init-job
    job: remindmebot-init
    tags: [apps, remindmebot]
  - name: remindmebot
    file: remindmebot/remindmebot.yaml
    tags: [apps, remindmebot]
  - name: blobstore-config
    inline: |
      apiVersion: v1
      data:
        database: ${BLOBSTORE_DATABASE_BASE64}
      kind: Secret
      metadata:
        name: blobstore-secrets
    parameters:
      - BLOBSTORE_DATABASE_BASE64
    tags: [apps, blobstore]
  - name: blobstore
    file: blobstore/blobstore.yaml
    tags: [apps, blobstore]
  - name: webcomics-config
    inline: |
      apiVersion: v1
      data:
        database: ${WEBCOMICS_DATABASE_BASE64}
      kind: Secret
      metadata:
        name: webcomics-secrets
    parameters:
      - WEBCOMICS_DATABASE_BASE64
    tags: [apps, webcomics]
  - name: webcomics
    file: webcomics/webcomics.yaml
    tags: [apps, webcomics]
  - name: syncthing
    file: syncthing/syncthing.yaml
    tags: [apps, syncthing]
  - name: bookstack-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${BOOKSTACK_MYSQL_USER}
        mysql_database: ${BOOKSTACK_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: bookstack-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${BOOKSTACK_MYSQL_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: bookstack-secrets
    parameters:
      - BOOKSTACK_MYSQL_USER
      - BOOKSTACK_MYSQL_DATABASE
      - BOOKSTACK_MYSQL_PASSWORD_BASE64
    tags: [apps, bookstack]
  - name: bookstack-init-job-create
    file: bookstack/bookstack-init.yaml
    tags: [apps, bookstack]
  - name: bookstack-init-job
    job: bookstack-mysql-init
    tags: [apps, bookstack]
  - name: bookstack
    file: bookstack/bookstack.yaml
    tags: [apps, bookstack]
  - name: bookstack-notifier-secrets
    inline: |
      apiVersion: v1
      data:
        token_id: ${BOOKSTACK_TOKEN_ID_BASE64}
        token_secret: ${BOOKSTACK_TOKEN_SECRET_BASE64}
      kind: Secret
      metadata:
        name: bookstack-notifier-secrets
    parameters:
      - BOOKSTACK_TOKEN_ID_BASE64
      - BOOKSTACK_TOKEN_SECRET_BASE64
    tags: [apps, bookstack]
  - name: bookstack-notifier
    file: bookstack/bookstack-notifier.yaml
    fileParameters:
      - BOOKSTACK_NOTIFIER_SCRIPT=bookstack/bookstack-notifier.sh
    tags: [apps, bookstack]
  - name: drone-init
    file: drone/drone-init.yaml
    tags: [apps, drone]
  - name: drone-init-job
    job: drone-postgres-init
    tags: [apps, drone]
  - name: drone-config
    inline: |
      apiVersion: v1
      data:
        connection_string: ${DRONE_CONNECTION_STRING_BASE64}
        rpc_secret: ${DRONE_RPC_SECRET_BASE64}
        gitea_client_id: ${DRONE_GITEA_CLIENT_ID_BASE64}
        gitea_client_secret: ${DRONE_GITEA_CLIENT_SECRET_BASE64}
      kind: Secret
      metadata:
        name: drone-secrets
    parameters:
      - DRONE_CONNECTION_STRING_BASE64
      - DRONE_RPC_SECRET_BASE64
      - DRONE_GITEA_CLIENT_ID_BASE64
      - DRONE_GITEA_CLIENT_SECRET_BASE64
    tags: [apps, drone]
  - name: drone
    file: drone/drone.yaml
    tags: [apps, drone]
  # endregion
  - name: pod-killer
    file: pod-killer/pod-killer.yaml
    parameters:
      - POD_KILLER_NAMESPACE=default
      - SLACK_BOT_ALERTING_CHANNEL
    fileParameters:
      - POD_KILLER_SCRIPT=pod-killer/pod-killer.sh
  - name: certbot-update-kubernetes-dashboard
    file: certbot/certbot-generic-cron.yaml
    parameters:
      - KUBERNETES_NAMESPACE=kubernetes-dashboard
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=certbot/certbot-copy-script.sh
    tags: [crons, certbot]
  - name: certbot-update-monitoring
    file: certbot/certbot-generic-cron.yaml
    parameters:
      - KUBERNETES_NAMESPACE=monitoring
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=certbot/certbot-copy-script.sh
    tags: [crons, certbot]
  - name: certbot-update-default
    file: certbot/certbot-generic-cron.yaml
    parameters:
      - KUBERNETES_NAMESPACE=default
      - INCLUDE_EXTERNAL_CERTS=true
      - INCLUDE_BARE_DOMAIN=true
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=certbot/certbot-copy-script.sh
    tags: [crons, certbot]
  - name: certbot-cron
    file: certbot/certbot.yaml
    fileParameters:
      - DNS_RENEW_SCRIPT=certbot/certbot-dns-renew.sh
    tags: [crons, certbot]
  # region: Scheduled rsync Jobs
  # This block contains all the scheduled jobs that run rsync against the
  #   various directories things are written to.
  # The primary objective tends to be creating copies in a consolidated place
  #   where a syncthing shared folder can back up the contents to a remote
  #   device.
  - name: rsync-image
    build:
      path: crons
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/rsync:latest
    tags: [backups]
  - name: bookstack-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=bookstack
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync --exclude=log
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/bookstack
      - SOURCE_PATH=/
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup/apps
      - DEST_PATH=/bookstack
    tags: [bookstack, backups]
  - name: node-red-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=node-red
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/node-red
      - SOURCE_PATH=/flows.json
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup
      - DEST_PATH=/node-red/
    tags: [nodered, backups]
  - name: gitea-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=gitea
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync --exclude=gitea/log
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/gitea
      - SOURCE_PATH=/
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup
      - DEST_PATH=/gitea
    tags: [gitea, backups]
  - name: bitwarden-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=bitwarden
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/bitwarden
      - SOURCE_PATH=/
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup/apps
      - DEST_PATH=/bitwarden
    tags: [bitwarden, backups]
  # endregion
  - name: shell-monitor-base-default
    file: shell-monitor/shell-monitor-base.yaml
    parameters:
      - SHELL_MONITOR_NAMESPACE=default
      - SLACK_BOT_ALERTING_CHANNEL
    fileParameters:
      - SHELL_MONITOR_SCRIPT=shell-monitor/shell-monitor-script.sh
    tags: [shell-monitor]
  - name: events-shell-monitor
    file: shell-monitor/events-shell-monitor.yaml
    fileParameters:
      - EVENTS_SHELL_MONITOR_SCRIPT=shell-monitor/events-shell-monitor.sh
    tags: [shell-monitor]
  - name: delete-manual-jobs-shell-monitor
    file: shell-monitor/delete-manual-jobs-monitor.yaml
    parameters:
      - KUBE_NAMESPACE=default
    fileParameters:
      - JOBS_SHELL_MONITOR_SCRIPT=shell-monitor/delete-manual-jobs-monitor.sh
    tags: [shell-monitor]
  - name: dev-namespace
    inline: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: dev
    tags: [rotate-node]
  - name: rotate-node-tagger
    file: node-rotator/rotate-node-tagger.yaml
    fileParameters:
      - ROTATE_NODE_TAGGER_SCRIPT=node-rotator/rotate-node-tagger.sh
    tags: [rotate-node]
jobs:
  - name: mysql-restore
    file: mysql/tasks/mysql-restore.yaml
    parameters:
      - MYSQL_DATABASE_NAME
  - name: mysql-drop
    file: mysql/tasks/mysql-drop.yaml
    parameters:
      - MYSQL_DATABASE_NAME
  - name: mongodb-restore
    file: mongodb/tasks/mongodb-restore.yaml
    parameters:
      - MONGODB_DATABASE_NAME
