# Pile of anchors to use just so configurable values are all kept in one spot.
# Could eventually be replaced with something like a constants/variables key
#   that could live here, but this does what it needs to.
_:
  - &log_level trace
  - &metallb_address_range ADDRESS_RANGE=192.168.200.32-192.168.200.224
  - &ingress_controller_service_ip LOAD_BALANCER_IP=192.168.200.64
  - &ingress_controller_external_service_ip LOAD_BALANCER_IP=192.168.200.129
  - &plex_load_balancer_service_ip LOAD_BALANCER_IP=192.168.200.69
  - &gitea_load_balancer_service_ip LOAD_BALANCER_IP=192.168.200.126
  - &wireguard_load_balancer_service_ip LOAD_BALANCER_IP=192.168.200.123
  - &pihole_ingress_controller_ip INGRESS_IP=192.168.200.64
  - &unifi_controller_service_ip LOAD_BALANCER_IP=192.168.200.82
  - &unifi_controller_inform_url http://192.168.200.82:8080
  - &devbox_service_ip LOAD_BALANCER_IP=192.168.200.224
  - &vm_builder_service_ip LOAD_BALANCER_IP=192.168.200.222
  - &pihole_load_balancer_service_ip LOAD_BALANCER_IP=192.168.200.217
  - &docker_hub_upstream_pull if-not-present
  - &local_upstream_pull always
access_point_controller: *unifi_controller_inform_url
access_points:
  - ubnt@192.168.1.86
  - ubnt@192.168.1.109
  - ubnt@192.168.1.152
vms:
  root: vms
  cache: /var/lib/packer/cache
  output: /var/lib/packer/images
  images:
    - name: load-balancer
      hypervisors: [beast1]
      parameters:
        - ESXI_USERNAME=root
        - ESXI_ROOT_PASSWORD
        - SERVER_SSH_KEYS_DIR=/var/lib/packer/etc/ssh
        - ESXI_NETWORK=VM Network
        - VM_ROOT_PASSWORD
        - VM_MANAGEMENT_PASSWORD
        - LOAD_BALANCER_IP
    - name: kubernetes-node-12.6.0-1.22.17
      hypervisors: [beast1]
      parameters:
        - ESXI_USERNAME=root
        - ESXI_ROOT_PASSWORD
        - SERVER_SSH_KEYS_DIR=/var/lib/packer/etc/ssh
        - ESXI_NETWORK=VM Network
        - VM_ROOT_PASSWORD
        - VM_MANAGEMENT_PASSWORD
        - LOAD_BALANCER_IP
    - name: kubernetes-node-12.6.0-1.23.17
      hypervisors: [beast1]
      parameters:
        - ESXI_USERNAME=root
        - ESXI_ROOT_PASSWORD
        - SERVER_SSH_KEYS_DIR=/var/lib/packer/etc/ssh
        - ESXI_NETWORK=VM Network
        - VM_ROOT_PASSWORD
        - VM_MANAGEMENT_PASSWORD
        - LOAD_BALANCER_IP
        - DEBIAN_VERSION=12.6.0
        - DEBIAN_SHA512=712cf43c5c9d60dbd5190144373c18b910c89051193c47534a68b0cd137c99bd8274902f59b25aba3b6ba3e5bca51d7c433c06522f40adb93aacc5e21acf57eb
        - KUBERNETES_VERSION=1.23.17
load_balancer_host: api.internal.aleemhaji.com
nodes:
  - name: beast1
    role: hypervisor
    engine: esxi
    host: 192.168.10.40
    user: root
    datastore: Main
    network: Kubernetes Network
  - name: beast2
    role: hypervisor
    engine: esxi
    host: 192.168.10.41
    user: root
    datastore: Main
    network: Kubernetes Network
  - name: home-load-balancer
    role: load-balancer
    hypervisor: beast1
    cpu: 1
    memory: 256
    user: packer
  - name: home-master-01
    role: master
    hypervisor: beast1
    cpu: 2
    memory: 4096
    user: packer
  - name: home-master-02
    role: master
    hypervisor: beast1
    cpu: 2
    memory: 4096
    user: packer
  - name: home-master-03
    role: master
    hypervisor: beast1
    cpu: 2
    memory: 4096
    user: packer
  - name: home-node-01
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  - name: home-node-02
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  - name: home-node-03
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  - name: home-node-04
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  - name: home-node-05
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  - name: home-node-06
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  # Provide extra capacity while cycling other nodes.
  - name: home-node-temp
    role: node
    hypervisor: beast1
    cpu: 2
    memory: 8192
    user: packer
  - name: home-master-temp
    role: master
    hypervisor: beast1
    cpu: 2
    memory: 4096
    user: packer
loglevel: *log_level
pod_network_cidr: 10.244.0.0/16
resources:
  # region: Namespaces
  - name: default-namespace
    file: infra/namespace.yaml
    parameters:
      - NAMESPACE=default
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
      - SLACK_BOT_ALERTING_CHANNEL
      - INCLUDE_EXTERNAL_CERTS=true
      - INCLUDE_BARE_DOMAIN=true
      - POD_KILLER_CONTAINER_RESTART_LIMIT=10
      - JOB_RETENTION_WINDOW=1 month
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=infra/certbot-copy-script.sh
      - POD_KILLER_SCRIPT=infra/pod-killer-script.sh
      - JOBS_SHELL_MONITOR_SCRIPT=infra/delete-manual-jobs-script.sh
    tags: [namespaces]
  - name: dev-namespace
    file: infra/namespace.yaml
    parameters:
      - NAMESPACE=dev
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
      - SLACK_BOT_ALERTING_CHANNEL
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
      - POD_KILLER_CONTAINER_RESTART_LIMIT=10
      - JOB_RETENTION_WINDOW=1 month
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=infra/certbot-copy-script.sh
      - POD_KILLER_SCRIPT=infra/pod-killer-script.sh
      - JOBS_SHELL_MONITOR_SCRIPT=infra/delete-manual-jobs-script.sh
    tags: [namespaces, rotate-node]
  - name: monitoring-namespace
    file: infra/namespace.yaml
    parameters:
      - NAMESPACE=monitoring
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
      - SLACK_BOT_ALERTING_CHANNEL
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
      - POD_KILLER_CONTAINER_RESTART_LIMIT=10
      - JOB_RETENTION_WINDOW=1 month
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=infra/certbot-copy-script.sh
      - POD_KILLER_SCRIPT=infra/pod-killer-script.sh
      - JOBS_SHELL_MONITOR_SCRIPT=infra/delete-manual-jobs-script.sh
    tags: [namespaces, monitoring]
  - name: kube-system-namespace
    file: infra/namespace.yaml
    parameters:
      - NAMESPACE=kube-system
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
      - SLACK_BOT_ALERTING_CHANNEL
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
      - POD_KILLER_CONTAINER_RESTART_LIMIT=10
      - JOB_RETENTION_WINDOW=1 month
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=infra/certbot-copy-script.sh
      - POD_KILLER_SCRIPT=infra/pod-killer-script.sh
      - JOBS_SHELL_MONITOR_SCRIPT=infra/delete-manual-jobs-script.sh
    tags: [namespaces]
  - name: kubernetes-dashboard-namespace
    file: infra/namespace.yaml
    parameters:
      - NAMESPACE=kubernetes-dashboard
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
      - SLACK_BOT_ALERTING_CHANNEL
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
      - POD_KILLER_CONTAINER_RESTART_LIMIT=10
      - JOB_RETENTION_WINDOW=1 month
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=infra/certbot-copy-script.sh
      - POD_KILLER_SCRIPT=infra/pod-killer-script.sh
      - JOBS_SHELL_MONITOR_SCRIPT=infra/delete-manual-jobs-script.sh
    tags: [namespaces, dashboard]
  - name: tasks-namespace
    file: infra/namespace.yaml
    parameters:
      - NAMESPACE=tasks
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_CONFIG_JSON_FILE_CONTENTS_BASE64
      - SLACK_BOT_ALERTING_CHANNEL
      - INCLUDE_EXTERNAL_CERTS=false
      - INCLUDE_BARE_DOMAIN=false
      - POD_KILLER_CONTAINER_RESTART_LIMIT=10
      - JOB_RETENTION_WINDOW=1 month
    fileParameters:
      - UPDATE_SECRETS_SCRIPT=infra/certbot-copy-script.sh
      - POD_KILLER_SCRIPT=infra/pod-killer-script.sh
      - JOBS_SHELL_MONITOR_SCRIPT=infra/delete-manual-jobs-script.sh
    tags: [namespaces, rmq, tasks]
  # endregion
  - name: node-setup
    file: infra/cluster/node-setup.yaml
  - name: calico
    file: infra/cluster/calico.yaml
    tags: [network]
  - name: kubernetes-dashboard-helm
    helm:
      namespace: kubernetes-dashboard
      repo: kubernetes-dashboard
      name: kubernetes-dashboard
      path: https://kubernetes.github.io/dashboard/
      chart: kubernetes-dashboard/kubernetes-dashboard
      version: "7.11.1"
  - name: kubernetes-dashboard-ingress
    file: infra/services/kubernetes-dashboard-ingress.yaml
    tags: [apps, dashboard]
  - name: metrics-server
    file: metrics-server.yaml
  - name: personal-service-account
    inline: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: aleem
        namespace: kube-system
      ---
      apiVersion: v1
      kind: Secret
      metadata:
        name: aleem
        namespace: kube-system
        annotations:
          kubernetes.io/service-account.name: aleem
      type: kubernetes.io/service-account-token
      ---
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: aleem
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: cluster-admin
      subjects:
      - kind: ServiceAccount
        name: aleem
        namespace: kube-system
    tags: [users]
  - name: load-balancer
    file: infra/cluster/metallb.yaml
    tags: [network, load-balancer]
  - name: load-balancer-config
    inline: |
      apiVersion: metallb.io/v1beta1
      kind: IPAddressPool
      metadata:
        name: service-pool
        namespace: metallb-system
      spec:
        addresses:
        - ${ADDRESS_RANGE}
      ---
      apiVersion: metallb.io/v1beta1
      kind: L2Advertisement
      metadata:
        name: service-advertisement
        namespace: metallb-system
      ---
      apiVersion: v1
      data:
        secretkey: "${METALLB_SYSTEM_MEMBERLIST_SECRET_KEY_BASE64}"
      kind: Secret
      metadata:
        name: memberlist
        namespace: metallb-system
    parameters:
      - METALLB_SYSTEM_MEMBERLIST_SECRET_KEY_BASE64
      - *metallb_address_range
    tags: [network, load-balancer]
  - name: ingress-controller
    file: infra/cluster/nginx-ingress-daemonset-plus-tcp-udp-proxy.yaml
    tags: [network, ingress, ingress-internal]
  - name: ingress-controller-external
    file: infra/cluster/nginx-ingress-daemonset-external.yaml
    tags: [network, ingress, ingress-external]
  - name: ingress-controller-configs
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: tcp-services
      data:
        22: "default/gitea:22"
        53: "default/pihole-tcp:53"
        3306: "default/mysql:3306"
        5432: "default/postgres:5432"
        27017: "default/mongodb:27017"
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: udp-services
      data:
        53: "default/pihole-udp:53"
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx
      data:
        http-redirect-code: "302"
        enable-real-ip: "true"
    tags: [network, ingress, ingress-internal]
  - name: ingress-controller-configs-external
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx-external
      data:
        http-redirect-code: "302"
        enable-real-ip: "true"
    tags: [network, ingress, ingress-external]
  - name: ingress-controller-service
    inline: |
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-ingress-tcp
        labels:
          app: nginx-ingress
        namespace: ingress-nginx
        annotations:
          metallb.universe.tf/allow-shared-ip: ingress
      spec:
        externalTrafficPolicy: Local
        ports:
        - port: 53
          name: dns
          protocol: TCP
        - port: 80
          name: http
          protocol: TCP
        - port: 443
          name: https
          protocol: TCP
        - port: 3306
          name: mysql
          protocol: TCP
        - port: 5432
          name: postgres
          protocol: TCP
        - port: 27017
          name: mongodb
          protocol: TCP
        - port: 51413
          name: osstransmission
          protocol: TCP
        type: LoadBalancer
        loadBalancerIP: ${LOAD_BALANCER_IP}
        selector:
          app.kubernetes.io/name: ingress-nginx
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-ingress-udp
        labels:
          app: nginx-ingress
        namespace: ingress-nginx
        annotations:
          metallb.universe.tf/allow-shared-ip: ingress
      spec:
        externalTrafficPolicy: Local
        ports:
        - port: 53
          name: dns
          protocol: UDP
        type: LoadBalancer
        loadBalancerIP: ${LOAD_BALANCER_IP}
        selector:
          app.kubernetes.io/name: ingress-nginx
    tags: [network, ingress, ingress-internal]
    parameters:
      - *ingress_controller_service_ip
  - name: ingress-controller-service-external
    inline: |
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-ingress-tcp
        labels:
          app: nginx-ingress
        namespace: ingress-nginx-external
        annotations:
          metallb.universe.tf/allow-shared-ip: ingress-external
      spec:
        externalTrafficPolicy: Local
        ports:
        - port: 80
          name: http
          protocol: TCP
        - port: 443
          name: https
          protocol: TCP
        type: LoadBalancer
        loadBalancerIP: ${LOAD_BALANCER_IP}
        selector:
          app.kubernetes.io/name: ingress-nginx
      # ---
      # apiVersion: v1
      # kind: Service
      # metadata:
      #   name: nginx-ingress-udp
      #   labels:
      #     app: nginx-ingress
      #   namespace: ingress-nginx-external
      #   annotations:
      #     metallb.universe.tf/allow-shared-ip: ingress-external
      # spec:
      #   externalTrafficPolicy: Local
      #   ports:
      #   type: LoadBalancer
      #   loadBalancerIP: ${LOAD_BALANCER_IP}
      #   selector:
      #     app.kubernetes.io/name: ingress-nginx
    tags: [network, ingress, ingress-external]
    parameters:
      - *ingress_controller_external_service_ip
  - name: docker-registry-htpasswd-secrets
    inline: |
      apiVersion: v1
      data:
        htpasswd: ${DOCKER_HTPASSWD_FILE_CONTENTS_BASE64}
      kind: Secret
      metadata:
        name: ${DOCKER_REGISTRY_HOSTNAME}-htpasswd
    parameters:
      - DOCKER_REGISTRY_HOSTNAME
      - DOCKER_HTPASSWD_FILE_CONTENTS_BASE64
    tags: [apps, registry]
  - name: docker-registry
    file: registry/registry.yaml
    tags: [apps, registry]
  # region: Docker Image Caching
  # At this point, the registry has been pushed, and all the appropriate
  #   ingress configurations should be set to allow for images to be cached.
  # Cache a bunch of images from Docker Hub to the local registry to prevent
  #   being rate limited during standard operations within the cluster.
  # The main contributor to Docker Hub pulls may just be the crons, but if the
  #   cluster has tried enough pulls within the last 6 hours, just doing
  #   rudimentary dev work gets blocked.
  # Rehosting the images on the cluster also helps steer things towards images
  #   being built from same base image any time the same tag appears, rather
  #   it being based on what base image the build host had at the time.
  # Similar issues are possible, but are a little more controlled.
  - name: ubuntu-1604-image-cache
    build:
      source: ubuntu:16.04
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/ubuntu:16.04
    tags: [docker-cache, crons]
  - name: ubuntu-2004-image-cache
    build:
      source: ubuntu:20.04
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/ubuntu:20.04
    tags: [docker-cache, transmission]
  - name: certbot-image-cache
    build:
      source: certbot/certbot:v3.2.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/certbot/certbot:v3.2.0
    tags: [docker-cache, certbot]
  - name: mongo-37-image-cache
    build:
      source: mongo:3.7
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/mongo:3.7
    tags: [docker-cache, mongodb]
  - name: mongo-36-image-cache
    build:
      source: mongo:3.6
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/mongo:3.6
    tags: [docker-cache, mongodb, unifi]
  - name: busybox-image-cache
    build:
      source: busybox:1.37.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/busybox:1.37.0
    tags: [docker-cache, mysql, pihole]
  - name: mysql-57-image-cache
    build:
      source: mysql:5.7.43
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/mysql:5.7.43
    tags: [docker-cache, mysql]
  - name: pihole-image-cache
    build:
      source: pihole/pihole:v5.8
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/pihole:v5.8
    tags: [docker-cache, pihole]
  - name: browser-image-cache
    build:
      source: klausmeyer/docker-registry-browser:1.7.5
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/docker-registry-browser:1.7.5
    tags: [docker-cache, browser]
  - name: postgres-image-cache
    build:
      source: postgres:9
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/postgres:9
    tags: [docker-cache, postgres]
  - name: firefly-image-cache
    build:
      source: fireflyiii/core:version-6.2.10
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/fireflyiii-core:version-6.2.10
    tags: [docker-cache, firefly]
  - name: firefly-data-importer-image-cache
    build:
      source: fireflyiii/data-importer:version-1.6.1
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/fireflyiii-data-importer:version-1.6.1
    tags: [docker-cache, firefly]
  - name: heimdall-image-cache
    build:
      source: linuxserver/heimdall:2.6.3
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/heimdall:2.6.3
    tags: [docker-cache, heimdall]
  - name: plex-image-cache
    build:
      source: plexinc/pms-docker:1.40.5.8921-836b34c27
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/pms-docker:1.40.5.8921-836b34c27
    tags: [docker-cache, plex]
  - name: jellyfin-image-cache
    build:
      source: jellyfin/jellyfin:10.10.6
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/jellyfin/jellyfin:10.10.6
    tags: [docker-cache, jellyfin]
  - name: guacamole-image-cache
    build:
      source: guacamole/guacamole:1.5.5
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/guacamole:1.5.5
    tags: [docker-cache, guacamole]
  - name: guacd-image-cache
    build:
      source: guacamole/guacd:1.5.5
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/guacd:1.5.5
    tags: [docker-cache, guacamole]
  - name: gitea-image-cache
    build:
      source: gitea/gitea:1.22.3-rootless
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/gitea:1.22.3-rootless
    tags: [docker-cache, gitea]
  - name: kubectl-image-cache
    build:
      source: bitnami/kubectl:1.21.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/kubectl:1.21.0
    tags: [docker-cache, kubectl, uptime-kuma]
  - name: bitwarden-image-cache
    build:
      source: vaultwarden/server:1.33.2
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/vaultwarden/server:1.33.2
    tags: [docker-cache, bitwarden, vaultwarden]
  - name: syncthing-image-cache
    build:
      source: syncthing/syncthing:1.27.10
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/syncthing:1.27.10
    tags: [docker-cache, syncthing]
  - name: drone-image-cache
    build:
      source: drone/drone:2.26.0
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/drone/drone:2.26.0
    tags: [docker-cache, drone]
  - name: drone-agent-image-cache
    build:
      source: drone/drone-runner-kube:1.0.0-rc.5
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/drone/drone-runner-kube:1.0.0-rc.5
    tags: [docker-cache, drone]
  - name: wireguard-image-cache
    build:
      source: masipcat/wireguard-go:0.0.20230223
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/wireguard-go:0.0.20230223
    tags: [docker-cache, wireguard]
  - name: unifi-controller-image-cache
    build:
      source: linuxserver/unifi-controller:7.4.162
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/unifi-controller:7.4.162
    tags: [docker-cache, unifi]
  - name: uptime-kuma-image-cache
    build:
      source: louislam/uptime-kuma:1.23.15-alpine
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/louislam/uptime-kuma:1.23.15-alpine
    tags: [docker-cache, uptime-kuma]
  - name: nginx-image-cache
    build:
      source: nginx:1.27.1-alpine
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/nginx:1.27.1-alpine
    tags: [docker-cache, knowledge]
  - name: rabbitmq-image-cache
    build:
      source: rabbitmq:3.13.7-management-alpine
      pull: *docker_hub_upstream_pull
      tag: registry.internal.aleemhaji.com/rabbitmq:3.13.7-management-alpine
    tags: [docker-cache, rmq]
  # endregion
  - name: pihole
    file: pihole/pihole.yaml
    parameters:
      - *pihole_ingress_controller_ip
      - *pihole_load_balancer_service_ip
    tags: [apps, pihole]
  # https://github.com/prometheus-operator/kube-prometheus#quickstart
  - name: prometheus
    file: opsbox.yaml
    parameters:
      - JOB_NAME=deploy-prometheus
      - SCRIPT=cd /tmp; curl -fsSL https://github.com/prometheus-operator/kube-prometheus/archive/v0.10.0.tar.gz | tar xvz; cd kube-prometheus-0.10.0; kubectl create -f manifests/setup; until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ""; done; kubectl create -f manifests;
    tags: [monitoring]
  - name: prometheus-wait
    job: kube-system/ops-deploy-prometheus
    tags: [monitoring]
  - name: grafana
    file: grafana/grafana.yaml
    tags: [monitoring]
  # region: Stateless Services
  # The set of services in this block are services that don't rely on storing
  #   any state on disk.
  # These are grouped together so that they can be easily be pushed out to
  #   parallel clusters first without having any concern for services fighting
  #   over shares/volumes.
  # They should each have the stateless tag applied to them.
  - name: image-monitor-image
    build:
      path: image-monitor
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/image-monitor:latest
    tags: [apps, stateless, image-monitor]
  - name: image-monitor-config
    inline: |
      apiVersion: v1
      binaryData:
        image-monitor.sh: ${IMAGE_MONITOR_SCRIPT}
        image-monitor-ignore.json: ${IMAGE_MONITOR_IGNORE_JSON}
      kind: ConfigMap
      metadata:
        name: image-monitor-config
    fileParameters:
      - IMAGE_MONITOR_SCRIPT=./image-monitor/image-monitor.sh
      - IMAGE_MONITOR_IGNORE_JSON=./image-monitor/image-monitor-ignore.json
    tags: [crons, image-monitor, stateless]
  - name: image-monitor
    file: image-monitor/image-monitor.yaml
    parameters:
      - SLACK_BOT_ALERTING_CHANNEL
    tags: [crons, image-monitor, stateless]
  - name: browser-secrets
    inline: |
      apiVersion: v1
      data:
        key_base: ${DOCKER_REGISTRY_BROWSER_SECRET_KEY_BASE_BASE64}
      kind: Secret
      metadata:
        name: registry-browser-secrets
    parameters:
      - DOCKER_REGISTRY_BROWSER_SECRET_KEY_BASE_BASE64
  - name: browser
    file: browser/browser.yaml
    tags: [apps, browser, stateless]
  - name: slackbot-image
    build:
      path: slackbot
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/slackbot:latest
    tags: [apps, slackbot, stateless]
  - name: slackbot-config
    inline: |
      apiVersion: v1
      data:
        alerting_channel: ${SLACK_BOT_ALERTING_CHANNEL}
        default_channel: ${SLACK_BOT_DEFAULT_CHANNEL}
      kind: ConfigMap
      metadata:
        name: slack-bot-config
      ---
      apiVersion: v1
      data:
        api_key: ${SLACK_BOT_API_KEY_BASE64}
      kind: Secret
      metadata:
        name: slack-bot-secrets
    parameters:
      - SLACK_BOT_ALERTING_CHANNEL
      - SLACK_BOT_DEFAULT_CHANNEL
      - SLACK_BOT_API_KEY_BASE64
    tags: [apps, slackbot, stateless]
  - name: slackbot
    file: slackbot/slackbot.yaml
    tags: [apps, slackbot, stateless]
  - name: dns-update-secrets
    inline: |
      apiVersion: v1
      data:
        value: ${NAMESILO_API_KEY_BASE64}
      kind: Secret
      metadata:
        name: namesilo-api-key
    parameters:
      - NAMESILO_API_KEY_BASE64
    tags: [crons, dns, certbot, stateless]
  - name: dns-manager
    file: dns-operator.yaml
    tags: [dns, stateless]
  - name: latex-image
    build:
      path: latex
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/latex-server:latest
    tags: [apps, latex, stateless]
  - name: latex-server
    file: latex/latex.yaml
    tags: [apps, latex, stateless]
  # endregion
  # region: NFS-Share Only Services
  # These services are grouped because they have a pretty small number of
  #   dependencies outside the cluster itself.
  # These can easily be stopped in one cluster, and started in another without
  #   the need to coordinate ordering with other services in the cluster.
  # Disks management is also entirely outside the responsibility of these
  #   pods, so they're a bit lower maintenance.
  # This block should only list services that are internally-accessible.
  # Anything that's exposed publicly shouldn't be considered NFS-share only.
  - name: heimdall
    file: heimdall/heimdall.yaml
    tags: [apps, heimdall]
  - name: tedbot-config
    inline: |
      apiVersion: v1
      data:
        slack_webhook_url: ${SLACK_TEDBOT_APP_WEBHOOK_BASE64}
      kind: Secret
      metadata:
        name: tedbot-secrets
    parameters:
      - SLACK_TEDBOT_APP_WEBHOOK_BASE64
    tags: [crons, tedbot]
  - name: tedbot
    file: tedbot/tedbot.yaml
    tags: [crons, tedbot]
  - name: gitea
    file: gitea/gitea.yaml
    parameters:
      - *gitea_load_balancer_service_ip
    tags: [apps, gitea]
  - name: bitwarden-config
    inline: |
      apiVersion: v1
      data:
        admin_token: ${BITWARDEN_ADMIN_TOKEN_BASE64}
      kind: Secret
      metadata:
        name: bitwarden-secrets
    parameters:
      - BITWARDEN_ADMIN_TOKEN_BASE64
    tags: [apps, bitwarden, vaultwarden]
  - name: bitwarden
    file: bitwarden/bitwarden.yaml
    tags: [apps, bitwarden, vaultwarden]
  - name: uptime-kuma
    file: uptime-kuma/uptime-kuma.yaml
    tags: [apps, uptime-kuma]
  - name: uptime-kuma-backup
    file: uptime-kuma/uptime-kuma-backup.yaml
    fileParameters:
      - BACKUP_SCRIPT=uptime-kuma/backup.js
    tags: [apps, uptime-kuma]
  - name: knowledge-service
    file: barebones-nginx/service.yaml
    parameters:
      - NAME=knowledge
    tags: [apps, web, knowledge]
  - name: knowledge-ingress
    file: barebones-nginx/internal-ingress.yaml
    parameters:
      - NAME=knowledge
    tags: [apps, web, knowledge]
  # endregion
  # region: NFS-Share + Port Forwarded Services
  # This block of services include resources that need access to shared
  #   storage, but also have a port forward configured for them.
  # Deploying these resources will require a little bit of extra care, as they
  #   will be temporarily unreachable on whatever port they're talking
  #   externally through.
  # These shouldn't have any database dependencies, but can have NFS and
  #   ingress.
  - name: transmission-image
    build:
      path: transmission
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/transmission:2004
    tags: [apps, transmission]
  - name: osstransmission
    file: transmission/transmission.yaml
    tags: [apps, transmission]
  - name: plex
    file: plex/plex.yaml
    parameters:
      - *plex_load_balancer_service_ip
    tags: [apps, plex]
  - name: jellyfin
    file: jellyfin/jellyfin.yaml
    tags: [apps, jellyfin]
  - name: wireguard
    file: wireguard/wireguard.yaml
    parameters:
      - *wireguard_load_balancer_service_ip
      - WIREGUARD_SERVER_IP=172.27.224.1
      - WIREGUARD_CLIENT_IP_CIDR=172.27.224.0/24
      - WIREGUARD_SERVER_PRIVATE_KEY
      - WIREGUARD_PEER_1_PUBLIC_KEY
      - WIREGUARD_PEER_1_IP=172.27.224.32/32
    tags: [apps, wireguard]
  # endregion
  # region: Databases
  # This section contains definitions for databases.
  # These resources will typically need to refer to block storage that's been
  #   exposed via another piece of hardware that's outside the scope of hope,
  #   for now.
  # These resources will be consumed by others, so they appear in this file
  #   before any of the services that consume them.
  # The number of dependencies they have may make migrations a bit more of a
  #   challenge.
  - name: postgres-config
    inline: |
      apiVersion: v1
      data:
        root_password: ${POSTGRES_ROOT_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: postgres-secrets
    parameters:
      - POSTGRES_ROOT_PASSWORD_BASE64
    tags: [apps, postgres]
  - name: postgres
    file: postgres/postgres.yaml
    tags: [apps, postgres]
  - name: mysql-config
    inline: |
      apiVersion: v1
      data:
        root_password: ${MYSQL_ROOT_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: mysql-secrets
    parameters:
      - MYSQL_ROOT_PASSWORD_BASE64
    tags: [apps, mysql]
  - name: mysql-backup
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mysql-backup
      binaryData:
        mysql-backup.sh: ${MYSQL_BACKUP_SCRIPT}
    fileParameters:
      - MYSQL_BACKUP_SCRIPT=mysql/mysql-backup.sh
    tags: [apps, mysql, backups]
  - name: mysql
    file: mysql/mysql.yaml
    tags: [apps, mysql]
  - name: mongodb-backup
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: mongodb-backup
      binaryData:
        mongodb-backup.sh: ${MONGODB_BACKUP_SCRIPT}
    tags: [apps, mongodb, backups]
    fileParameters:
      - MONGODB_BACKUP_SCRIPT=mongodb/mongodb-backup.sh
  - name: mongodb
    file: mongodb/mongodb.yaml
    tags: [apps, mongodb]
  - name: rabbitmq-config
    inline: |
      apiVersion: v1
      kind: Secret
      metadata:
        name: rabbitmq-secrets
        namespace: tasks
      data:
        username: ${RMQ_USERNAME_BASE64}
        password: ${RMQ_PASSWORD_BASE64}
        .erlang-cookie: ${RMQ_PASSWORD_BASE64}
        connection_string: ${RMQ_CONNECTION_STRING_BASE64}
        management_connection_string: ${RMQ_MANAGEMENT_CONNECTION_STRING_BASE64}
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: rmq-bridge-autoscaler-script
        namespace: tasks
      binaryData:
        autoscaler.sh: ${AUTOSCALER_SCRIPT}
    parameters:
      - RMQ_USERNAME_BASE64
      - RMQ_PASSWORD_BASE64
      - RMQ_ERLANG_COOKIE_BASE64
      - RMQ_CONNECTION_STRING_BASE64
      - RMQ_MANAGEMENT_CONNECTION_STRING_BASE64
    fileParameters:
      - AUTOSCALER_SCRIPT=tasks/autoscaler.sh
    tags: [apps, rmq]
  - name: rabbitmq
    file: tasks/rabbitmq.yaml
    tags: [apps, rmq]
  - name: rabbitmq-http-bridge-init
    file: tasks/rmq-http-bridge-init.yaml
    tags: [apps, rmq]
  - name: rabbitmq-http-bridge-init-wait
    job: tasks/rabbitmq-delay-infrastructure-init
    tags: [apps, rmq]
  - name: rabbitmq-http-bridge
    file: tasks/rmq-http-bridge.yaml
    tags: [apps, rmq]
  - name: rabbitmq-http-bridge-autoscaler-image
    build:
      path: tasks
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/rmq-bridge-autoscaler:1.21.0
    tags: [apps, rmq]
  - name: rabbitmq-http-bridge-autoscaler
    file: tasks/rmq-http-bridge-autoscaler.yaml
    tags: [apps, rmq]
  # endregion
  # region: Database-Dependent Services
  # This section contains the listing of services that rely on some form of
  #   datastore outside of just file system.
  # This could come in the form of network references to resources defined in
  #   the databases section above, or in the form of a database defined in its
  #   own pod that requires a special ISCSi device set up for it.
  - name: unifi-controller-image
    build:
      path: unifi
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/unifi-controller:7.4.162-custom
    tags: [apps, unifi]
  - name: unifi-controller
    file: unifi/unifi.yaml
    parameters:
      - *unifi_controller_service_ip
    tags: [apps, unifi]
  - name: firefly-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${FIREFLY_MYSQL_USER}
        mysql_database: ${FIREFLY_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: firefly-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${FIREFLY_MYSQL_PASSWORD_BASE64}
        app_key: ${FIREFLY_APP_KEY_BASE64}
        importer_pat: ${FIREFLY_DATA_IMPORTER_KEY_BASE64}
      kind: Secret
      metadata:
        name: firefly-secrets
    parameters:
      - FIREFLY_MYSQL_USER
      - FIREFLY_MYSQL_DATABASE
      - FIREFLY_MYSQL_PASSWORD_BASE64
      - FIREFLY_APP_KEY_BASE64
      - FIREFLY_DATA_IMPORTER_KEY_BASE64
    tags: [apps, firefly]
  - name: firefly-init-job-create
    file: firefly/firefly-init.yaml
    tags: [apps, firefly]
  - name: firefly-init-job
    job: firefly-mysql-init
    tags: [apps, firefly]
  - name: firefly
    file: firefly/firefly.yaml
    tags: [apps, firefly]
  - name: firefly-data-importer
    file: firefly/firefly-data-importer.yaml
    tags: [apps, firefly]
  - name: guacamole-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${GUACAMOLE_MYSQL_USER}
        mysql_database: ${GUACAMOLE_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: guacamole-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${GUACAMOLE_MYSQL_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: guacamole-secrets
    parameters:
      - GUACAMOLE_MYSQL_USER
      - GUACAMOLE_MYSQL_DATABASE
      - GUACAMOLE_MYSQL_PASSWORD_BASE64
    tags: [apps, guacamole]
  - name: guacamole-init-job-create
    file: guacamole/guacamole-init.yaml
    tags: [apps, guacamole]
  - name: guacamole-init-job
    job: guacamole-mysql-init
    tags: [apps, guacamole]
  - name: guacamole
    file: guacamole/guacamole.yaml
    tags: [apps, guacamole]
  - name: node-red-config
    inline: |
      apiVersion: v1
      data:
        mysql_user: ${NODE_RED_MYSQL_USER}
        mysql_database: ${NODE_RED_MYSQL_DATABASE}
      kind: ConfigMap
      metadata:
        name: node-red-config
      ---
      apiVersion: v1
      data:
        mysql_password: ${NODE_RED_MYSQL_PASSWORD_BASE64}
      kind: Secret
      metadata:
        name: node-red-secrets
    parameters:
      - NODE_RED_MYSQL_USER
      - NODE_RED_MYSQL_DATABASE
      - NODE_RED_MYSQL_PASSWORD_BASE64
    tags: [apps, node-red]
  - name: node-red-image
    build:
      path: node-red
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/node-red:latest
    tags: [apps, node-red]
  - name: node-red-init-job-create
    file: node-red/node-red-init.yaml
    tags: [apps, node-red]
  - name: node-red-init-job
    job: node-red-mysql-init
    tags: [apps, node-red]
  - name: node-red
    file: node-red/node-red.yaml
    tags: [apps, node-red]
  - name: blobstore-config
    inline: |
      apiVersion: v1
      data:
        database: ${BLOBSTORE_DATABASE_BASE64}
      kind: Secret
      metadata:
        name: blobstore-secrets
    parameters:
      - BLOBSTORE_DATABASE_BASE64
    tags: [apps, blobstore]
  - name: blobstore
    file: blobstore/blobstore.yaml
    tags: [apps, blobstore]
  - name: syncthing
    file: syncthing/syncthing.yaml
    tags: [apps, syncthing]
  - name: drone-init
    file: drone/drone-init.yaml
    tags: [apps, drone]
  - name: drone-init-job
    job: drone-postgres-init
    tags: [apps, drone]
  - name: drone-config
    inline: |
      apiVersion: v1
      data:
        connection_string: ${DRONE_CONNECTION_STRING_BASE64}
        rpc_secret: ${DRONE_RPC_SECRET_BASE64}
        gitea_client_id: ${DRONE_GITEA_CLIENT_ID_BASE64}
        gitea_client_secret: ${DRONE_GITEA_CLIENT_SECRET_BASE64}
      kind: Secret
      metadata:
        name: drone-secrets
    parameters:
      - DRONE_CONNECTION_STRING_BASE64
      - DRONE_RPC_SECRET_BASE64
      - DRONE_GITEA_CLIENT_ID_BASE64
      - DRONE_GITEA_CLIENT_SECRET_BASE64
    tags: [apps, drone]
  - name: drone
    file: drone/drone.yaml
    tags: [apps, drone]
  # endregion
  - name: certbot-image
    build:
      path: certbot
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/certbot/certbot-custom:v3.2.0
    tags: [crons, certbot]
  - name: certbot-cloudflare-secrets
    inline: |
      apiVersion: v1
      kind: Secret
      metadata:
        name: cloudflare-dns
      data:
        api_token: ${CLOUDFLARE_DNS_API_KEY_BASE64}
    parameters:
      - CLOUDFLARE_DNS_API_KEY_BASE64
  - name: certbot-cloudflare-configs
    inline: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: cloudflare-dns
      data:
        zone_id: ${CLOUDFLARE_ZONE_ID}
    parameters:
      - CLOUDFLARE_ZONE_ID
  - name: certbot-cron
    file: certbot/certbot.yaml
    fileParameters:
      - DNS_RENEW_SCRIPT=certbot/certbot-dns-renew.sh
    tags: [crons, certbot]
  # region: Scheduled rsync Jobs
  # This block contains all the scheduled jobs that run rsync against the
  #   various directories things are written to.
  # The primary objective tends to be creating copies in a consolidated place
  #   where a syncthing shared folder can back up the contents to a remote
  #   device.
  - name: rsync-image
    build:
      path: crons
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/rsync:latest
    tags: [backups]
  - name: node-red-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=node-red
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/node-red
      - SOURCE_PATH=/flows.json
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup/apps
      - DEST_PATH=/node-red
    tags: [nodered, backups]
  - name: gitea-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=gitea
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync --exclude=gitea/log
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/gitea
      - SOURCE_PATH=/
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup/apps
      - DEST_PATH=/gitea
    tags: [gitea, backups]
  - name: bitwarden-backups
    file: crons/rsync-cron.yaml
    parameters:
      - CRONJOB_NAME=bitwarden
      - RSYNC_OPTIONS=-avhuDH --delete --exclude=.sync
      - SOURCE_NFS_SERVER=192.168.96.4
      - SOURCE_NFS_SHARE=/mnt/main/apps/bitwarden
      - SOURCE_PATH=/
      - DESTINATION_NFS_SERVER=192.168.96.4
      - DESTINATION_NFS_SHARE=/mnt/main/backup/apps
      - DEST_PATH=/bitwarden
    tags: [bitwarden, backups]
  # endregion
  - name: shell-monitor-base-default
    file: shell-monitor/shell-monitor-base.yaml
    parameters:
      - SHELL_MONITOR_NAMESPACE=default
    fileParameters:
      - SHELL_MONITOR_SCRIPT=shell-monitor/shell-monitor-script.sh
    tags: [shell-monitor]
  - name: events-shell-monitor
    file: shell-monitor/events-shell-monitor.yaml
    parameters:
      - SLACK_BOT_ALERTING_CHANNEL
    fileParameters:
      - EVENTS_SHELL_MONITOR_SCRIPT=shell-monitor/events-shell-monitor.sh
    tags: [shell-monitor]
  - name: delete-manual-jobs-shell-monitor
    file: shell-monitor/delete-manual-jobs-monitor.yaml
    parameters:
      - KUBE_NAMESPACE=default
      - SLACK_BOT_ALERTING_CHANNEL
    fileParameters:
      - JOBS_SHELL_MONITOR_SCRIPT=shell-monitor/delete-manual-jobs-monitor.sh
    tags: [shell-monitor]
  - name: home-network-image
    build:
      path: .
      pull: *local_upstream_pull
      tag: registry.internal.aleemhaji.com/home-network:latest
    tags: [rotate-node, build-vm]
  - name: vm-builder-static-resources
    file: jobs/build-vm-static.yaml
    parameters:
      - *vm_builder_service_ip
    tags: [build-vm]
  - name: dev-ssh-key
    inline: |
      apiVersion: v1
      data:
        id_rsa: "${DEV_SSH_KEY_BASE64}"
        id_rsa.pub: "${DEV_SSH_PUBLIC_KEY_BASE64}"
        known_hosts: "${DEV_KNOWN_HOSTS_BASE64}"
      kind: Secret
      metadata:
        name: dev-ssh-key
        namespace: "${SSH_KEY_NAMESPACE}"
    parameters:
      - SSH_KEY_NAMESPACE=dev
      - DEV_SSH_KEY_BASE64
      - DEV_SSH_PUBLIC_KEY_BASE64
      - DEV_KNOWN_HOSTS_BASE64
    tags: [rotate-node]
  - name: devbox
    file: devbox.yaml
    parameters:
      - *devbox_service_ip
    tags: [rotate-node]
  - name: rotate-node-tagger
    file: node-rotator/rotate-node-tagger.yaml
    fileParameters:
      - ROTATE_NODE_TAGGER_SCRIPT=node-rotator/rotate-node-tagger.sh
    tags: [rotate-node]
  - name: rotate-node-secrets
    inline: |
      apiVersion: v1
      kind: Secret
      metadata:
        name: rotate-node-secrets
        namespace: dev
      data:
        esxi_root_password: ${ESXI_ROOT_PASSWORD_BASE64}
        vm_root_password: ${VM_ROOT_PASSWORD_BASE64}
        vm_management_password: ${VM_MANAGEMENT_PASSWORD_BASE64}
    parameters:
      - ESXI_ROOT_PASSWORD_BASE64
      - VM_ROOT_PASSWORD_BASE64
      - VM_MANAGEMENT_PASSWORD_BASE64
  - name: rotate-node
    file: node-rotator/rotate-node.yaml
    parameters:
      - SLACK_BOT_ALERTING_CHANNEL
    tags: [rotate-node]
jobs:
  - name: mysql-restore
    file: mysql/tasks/mysql-restore.yaml
    parameters:
      - MYSQL_DATABASE_NAME
  - name: mysql-drop
    file: mysql/tasks/mysql-drop.yaml
    parameters:
      - MYSQL_DATABASE_NAME
  - name: mongodb-restore
    file: mongodb/tasks/mongodb-restore.yaml
    parameters:
      - MONGODB_DATABASE_NAME
  - name: rotate-node
    file: jobs/rotate-node.yaml
    parameters:
      - ROTATE_NODE_NAME
      - VM_IMAGE_NAME
      - SLACK_BOT_ALERTING_CHANNEL
  - name: build-vm
    file: jobs/build-vm.yaml
    parameters:
      - VM_NAME
      - SLACK_BOT_ALERTING_CHANNEL
      - *vm_builder_service_ip
